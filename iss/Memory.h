//
// Copyright (C) 2005 by Freescale Semiconductor Inc.  All rights reserved.
//
// You may distribute under the terms of the Artistic License, as specified in
// the COPYING file.
//

//
// The memory representation is a short, wide tree.  Internal nodes are pages
// of pointers to child nodes.  Leaf nodes are pages of bytes.  The depth of
// the tree is determined by the template parameter Total.  Through the use of
// recursive template types, we generate the internal node types, stopping at
// level 0, which is the leaf node (page of bytes).  The MemTree type is what
// is actually instantiated and is a wrapper which instantiates the Node
// template with the proper parameters.
//
// So, for example, MemTree<64,4> will create a tree of four levels of 16 bits
// each; three internal nodes and the addressing within the 64K page.
//
// MemTree<64,3> creates a three-level tree: 22 bits for the internal nodes (4
// Meg) and 20 bits for the page size (1 Meg).
//
// This file also contains wrappers which convert misaligned accesses to multiple
// aligned accesses.
//
//
// Currently, we handle endianness issues in two ways: We can either store all
// data in a big-endian format, requiring byte-swapping, or we can be data
// invariant and do address swizzling to store data.  Byte-swapping is actually
// pretty efficent, since we use the x86 bswap instruction.  However, TLM2
// requires data-invariant store with address swizzling, so that's why we
// support it.
//
// In that approach, data is stored in a little-endian format, but the address
// is modified so that reads and writes are consistent with a big-endian model.
// Since the largest atomic access we support is 64-bits, these are written with
// no modification.  All other accesses have their addresses modified within the
// 64-bit region.  This is similar to PPC Classic little-endian: You invert the
// bottom three bits, then subtract the access size.  Thus, a 32-bit write to
// 0x1000 actually is written to 0x1004.  A byte read to 0x0 is actually read
// from 0x7, etc.
//


//
// When using this in a multi-threaded simulation kernel, you must define
// THREAD_SAFE_MEMORY, which creates a mutex in order to ensure that the memory
// is thread safe.  Note, however, that the individual memory writes themselves
// are not protected, e.g. write32, just the updating of the sparse memory.  The
// models themselves (software running on the model) must add the appropriate
// locks.
//

#ifndef _MEMORY_H_
#define _MEMORY_H_

#include <stdexcept>
#include <sstream>
#include <string.h>
#include <assert.h>
#ifndef _MSC_VER
# include <strings.h>
#endif

#ifdef THREAD_SAFE_MEMORY
# include <atomic>
# include <mutex>
# include "iss/ThreadedSimLock.h"
#endif

// If we're not compiling for Windows then we include the configuration file
// that's generated by autoconf.  Otherwise, we assume little-endian, since
// Windows only runs on x86.
#ifndef _MSC_VER
# include "adl_config.h"
#endif

#include "helpers/BasicTypes.h"
#include "helpers/Macros.h"
#include "helpers/BitTwiddles.h"

// If defined, various memory settings will be printed to standard-out.
//#define PrintMemStates

namespace adl {

  // Byte swapping routines.  Note: Newer versions of gcc define
  // __builtin_bswap16, but that's not present in the 4.4.4 series.
  inline uint16_t swab16(uint16_t x) 
  {
# if defined (_MSC_VER)
  return _byteswap_ushort(x);
# else
	union {
		uint16_t x;
		struct {
			uint8_t a;
			uint8_t b;
		} s;
	} in, out;
	in.x = x;
	out.s.a = in.s.b;
	out.s.b = in.s.a;
	return out.x;
# endif
  }

  inline uint32_t swab32(uint32_t x) 
  {
# if defined (_MSC_VER)
  return _byteswap_ulong(x);
# elif !defined(__GNUC__) || defined(USE_NO_GCC_BUILTINS)  
	union {
		uint32_t x;
		struct {
			uint8_t a;
			uint8_t b;
			uint8_t c;
			uint8_t d;
		} s;
	} in, out;
	in.x = x;
	out.s.a = in.s.d;
	out.s.b = in.s.c;
	out.s.c = in.s.b;
	out.s.d = in.s.a;
	return out.x;
# elif defined (_MSC_VER)
  return _byteswap_ushort(x);
# else
  return __builtin_bswap32(x);  
# endif
  }

  inline uint64_t swab64(uint64_t x) 
  {
# if defined (_MSC_VER)
  return _byteswap_uint64(x);
# elif !defined(__GNUC__) || defined(USE_NO_GCC_BUILTINS)  
	union {
		uint64_t x;
		struct {
			uint8_t a;
			uint8_t b;
			uint8_t c;
			uint8_t d;
			uint8_t e;
			uint8_t f;
			uint8_t g;
			uint8_t h;
		} s;
	} in, out;
	in.x = x;
	out.s.a = in.s.h;
	out.s.b = in.s.g;
	out.s.c = in.s.f;
	out.s.d = in.s.e;
	out.s.e = in.s.d;
	out.s.f = in.s.c;
	out.s.g = in.s.b;
	out.s.h = in.s.a;
	return out.x;
# else
  return __builtin_bswap64(x);  
# endif
  }

  // These will swap bytes if necessary, as determined by autoconf.
  inline uint16_t be16(uint16_t a)
  {
# ifdef WORDS_BIGENDIAN
    return a;
# else
    return swab16(a);
# endif
  }

  inline uint32_t be32(uint32_t a)
  {
# ifdef WORDS_BIGENDIAN
    return a;
# else
    return swab32(a);
# endif
  }

  inline uint64_t be64(uint64_t a)
  {
# ifdef WORDS_BIGENDIAN
    return a;
# else
    return swab64(a);
# endif
  }

  //
  // Routines for reading/writing values, given a byte-pointer.  In the case of
  // writes, a mask is used to calculate what portion of the value should be
  // actually written to memory.
  //
  // These are the byte-swapping variants, if on a little-endian machine, which
  // are address invariant.
  //


# if !defined(DATA_INVARIANT_MEMORY)

  inline uint64_t byte_read64(const byte_t *p,unsigned offset)
  {
    uint64_t tmp = *((uint64_t*)&p[offset]);
    return be64(tmp);
  }

  inline uint32_t byte_read32(const byte_t *p,unsigned offset)
  {
    uint32_t tmp = *((uint32_t*)&p[offset]);
    return be32(tmp);
  }

  // This is always address-invariant (byte swaps if necessary) and is used, for
  // example, by ISS safe-mode uADL models within the prefetch buffer, where we
  // don't want to do address swapping.
  inline uint32_t byte_read32_addr_inv(const byte_t *p,unsigned offset)
  {
    return byte_read32(p,offset);
  }

  inline uint16_t byte_read16(const byte_t *p,unsigned offset)
  {
    uint16_t tmp = *((uint16_t*)&p[offset]);
    return be16(tmp);
  }

  inline uint8_t byte_read8(const byte_t *p,unsigned offset)
  {
    return p[offset];
  }

  inline void byte_write64(byte_t *p,unsigned offset,uint64_t orig_value,unsigned nb = BytesPerDW)
  {
    uint64_t value = be64(orig_value);
    if (nb == BytesPerDW) {
      *((uint64_t*)&p[offset]) = value;
    } else {
      memcpy(&p[offset],((byte_t*)&value),nb);
    }
  }

  inline void byte_write32(byte_t *p,unsigned offset,uint32_t orig_value,unsigned nb = BytesPerWord)
  {
    uint32_t value = be32(orig_value);
    if (nb == BytesPerWord) {
      *((uint32_t*)&p[offset]) = value;
    } else {
      memcpy(&p[offset],((byte_t*)&value),nb);
    }
  }

  inline void byte_write16(byte_t *p,unsigned offset,uint16_t orig_value,unsigned nb = BytesPerHW)
  {
    uint16_t value = be16(orig_value);
    if (nb == BytesPerWord) {
      *((uint16_t*)&p[offset]) = value;
    } else {
      memcpy(&p[offset],((byte_t*)&value),nb);
    }
  }

  inline void byte_write8(byte_t *p,unsigned offset,uint8_t orig_value)
  {
    p[offset] = orig_value;
  }

  template<unsigned NB>
  addr_t swizzle_addr(addr_t addr)
  {
    return addr;
  }

  template<unsigned NB>
  byte_t *swizzle_ptr(byte_t *p)
  {
    return p;
  }

# else

  //
  // These are the address-swizzling versions, which are data-invariant.
  //

  inline uint64_t byte_read64(const byte_t *buf,unsigned offset)
  {
    uint64_t y;
    buf = (byte_t*) ((size_t)buf+((offset ^ DWMask))) - DWMask;
    memcpy ((void*)&y,(void*)buf,sizeof(y));
    return y;
  }

  inline uint32_t byte_read32(const byte_t *buf,unsigned offset)
  {
    uint32_t y;
    buf = (byte_t*) ((size_t)buf+((offset ^ DWMask))) - WordMask;
    memcpy ((void*)&y,(void*)buf,sizeof(y));
    return y;
  }

  // This is always address-invariant (byte swaps if necessary) and is used, for
  // example, by ISS safe-mode uADL models within the prefetch buffer, where we
  // don't want to do address swapping.
  inline uint32_t byte_read32_addr_inv(const byte_t *p,unsigned offset)
  {
    return *((uint32_t*)&p[offset]);
  }

  inline uint16_t byte_read16(const byte_t *buf,unsigned offset)
  {
    uint16_t y;
    buf = (byte_t*) ((size_t)buf+((offset ^ DWMask))) - HWMask;
    memcpy ((void*)&y,(void*)buf,sizeof(y));
    return y;
  }

  inline uint8_t byte_read8(const byte_t *p,unsigned offset)
  {
    return p[offset ^ DWMask];
  }

  inline void byte_write64(byte_t *buf,unsigned offset,uint64_t x,unsigned nb = BytesPerDW)
  {
    buf = (byte_t*) ((size_t)buf+((offset ^ DWMask))) - DWMask;
    memcpy ((void*)(buf+BytesPerDW-nb),(void*)((byte_t*)&x+BytesPerDW-nb),nb);
  }

  inline void byte_write32(byte_t *buf,unsigned offset,uint32_t x,unsigned nb = BytesPerWord)
  {
    buf = (byte_t*) ((size_t)buf+((offset ^ DWMask))) - WordMask;
    memcpy ((void*)(buf+BytesPerWord-nb),(void*)((byte_t*)&x+BytesPerWord-nb),nb);
  }

  inline void byte_write16(byte_t *buf,unsigned offset,uint16_t x,unsigned nb = BytesPerHW)
  {
    buf = (byte_t*) ((size_t)buf+((offset ^ DWMask))) - HWMask;
    memcpy ((void*)(buf+BytesPerHW-nb),(void*)((byte_t*)&x+BytesPerHW-nb),nb);
  }

  inline void byte_write8(byte_t *p,unsigned offset,uint8_t orig_value)
  {
    p[offset ^ DWMask] = orig_value;
  }

  template<unsigned NB>
  addr_t swizzle_addr(addr_t addr)
  {
    switch (NB) {
    case 8:
      return (addr ^ DWMask) - DWMask;
    case 4:
      return (addr ^ DWMask) - WordMask;
    case 2:
      return (addr ^ DWMask) - HWMask;
    case 1:
      return (addr ^ DWMask);
    default:
      assert(0);
    }
  }

  template<unsigned NB>
  byte_t *swizzle_ptr(byte_t *p)
  {
    switch (NB) {
    case 8:
      return (byte_t*)((size_t)p ^ DWMask) - DWMask;
    case 4:
      return (byte_t*)((size_t)p ^ DWMask) - WordMask;
    case 2:
      return (byte_t*)((size_t)p ^ DWMask) - HWMask;
    case 1:
      return (byte_t*)((size_t)p ^ DWMask);
    default:
      assert(0);
    }
  }

# endif

  //
  // These routines copy or fill a byte buffer of the specified size.
  //

  inline void byte_pagecopy(byte_t *trg,uint32_t toffset,const byte_t *src,uint32_t soffset,size_t n)
  {
    memcpy((void*)&trg[toffset],(void*)&src[soffset],n);
  }

  inline void byte_pagefill(byte_t *trg,uint32_t toffset,unsigned char fill,size_t n)
  {
    memset((void*)&trg[toffset],fill,n);
  }

  //
  // Defines various important constants as enumerations.
  //

  template <size_t BitWidth,size_t Total,size_t Level>
  struct NodeEnums {
    enum {
      Bits  = (BitWidth / (Total+1)),
      Size  = (1 << Bits),
      Mask  = (Size-1),

      PageBits = (Bits + (BitWidth % (Total+1))),
      PageSize = (1 << PageBits),
      PageMask = (PageSize-1),

      Shift = (Bits * (Level-1)) + PageBits,
    };
  };

  //
  // The m-ary tree used for the sparse memory array.
  //

  // An inner node in the m-ary tree.  Defines a child-node type and
  // routines for selecting the proper child node.
  //
  // For MT simulation, we implement a lockless update policy.  If we come
  // across a null node, then we try to update it with a newly allocated item
  // using compare-and-exchange (CAS).  If that succeeds, then we're fine- we
  // return a pointer to that new item.  If we fail, then we know that somebody
  // else succeeded in setting the new page, so we use that new page ourselves
  // and free the original.  This depends upon having a reasonably good malloc,
  // so that a new and then a delete are pretty fast.  Modern mallocs are pretty
  // good at this sort of thing, so I don't think it's necessary to
  // over-optimize with some sort of a free list.
  template <size_t BitWidth,size_t Total,size_t Level>
  struct Node {
    typedef NodeEnums<BitWidth,Total,Level> Enums;
    typedef Node<BitWidth,Total,Level-1> Child;

    Node() 
    { 
      memset(this,0,Enums::Size*sizeof(Child*)); 
    };

    ~Node()
    {
      for (int i = 0; i != Enums::Size; ++i) {
        delete _mem[i];
        // Safety, to avoid possible double deletion if this is instantiated as
        // a global w/shared objects.
        _mem[i] = 0;
      }
    }

    byte_t *getPage(addr_t addr)
    {
      return get(addr)->getPage(addr);
    }

#   ifdef THREAD_SAFE_MEMORY

    // Thread-safe version which uses atomics.
    Child *get(addr_t addr)
    {
      uint32_t index = ((addr >> Enums::Shift) & Enums::Mask);

      // Atomically load the pointer.
      Child *n = _mem[index].load(std::memory_order_consume);
      if (n == nullptr) {
        // Doesn't look like it's here, so try to set it.
        Child *new_item = new Child;
        if (_mem[index].compare_exchange_weak(n,new_item,std::memory_order_release,std::memory_order_relaxed)) {
          // We succeeded in setting it, so this is our new page.
          return new_item;
        } else {
          // Somebody else allocated it, so we don't need ours and can free it.
          delete new_item;
        }
      }
      return n;
    }

    std::atomic<Child *> _mem[Enums::Size];
    
#   else

    // Non-thread-safe version w/no atomics or locks.
    Child *get(addr_t addr)
    {
      uint32_t index = ((addr >> Enums::Shift) & Enums::Mask);
      Child *n = _mem[index];
      if (!n) {
        n = _mem[index] = new Child;
      }
      return n;
    }

    Child *         _mem[Enums::Size];
    
#   endif

  };

  // Partial specialization for level 0.  This is a leaf node in the tree and
  // contains the pages of bytes which store actual data.
  template <size_t BitWidth,size_t Total>
  struct Node<BitWidth,Total,0> {
    typedef NodeEnums<BitWidth,Total,0> Enums;

    Node() 
    { 
      memset(_mem,0,Enums::PageSize); 
    };

    byte_t *getPage(addr_t addr)
    {
      return _mem;
    }

    byte_t _mem[Enums::PageSize];
  };

  // Outermost node- provides an interface that hides some inner complexity.  This
  // is what should be instantiated by a class wishing to use the m-ary tree.
  // The access functions provided here only handle aligned addresses.
  template <size_t BitWidth,size_t Levels>
  struct SparseMemory : private Node<BitWidth,Levels-1,Levels-1> {

#   ifdef EXTRA_THREAD_SAFE_MEMORY
#     define EXTRA_SCOPED_LOCK scoped_lock my_lock(_mutex);
#   else
#     define EXTRA_SCOPED_LOCK
#   endif

    typedef Node<BitWidth,Levels-1,Levels-1> Base;
    typedef typename Node<BitWidth,Levels-1,Levels-1>::Enums Enums;

    static uint32_t pageOffset(addr_t addr) { return (addr & Enums::PageMask); };

    SparseMemory()
    {
      resetCache();
#     ifdef PrintMemStats
      std::cout << "Memory Settings:\n"
                << "  Bit width:      "   << std::dec << BitWidth << "\n"
                << "  Lookup levels:  "               << Levels   << "\n"
                << "  Node size:      0x" << std::hex << Enums::Size     << " (" << std::dec << Enums::Bits     << " bits)\n"
                << "  Page size:      0x" << std::hex << Enums::PageSize << " (" << std::dec << Enums::PageBits << " bits)\n"
                << "\n";
#     endif
    }

    enum {
      PageSize = Enums::PageSize,
      PageMask = Enums::PageMask,
    };

    void reset()
    {
      for (int i = 0; i != Enums::Size; ++i) {
        delete Base::_mem[i];
        Base::_mem[i] = 0;
      }
      resetCache();
    }

    // This has no effect on the sparse memory, since it can always handle a
    // full 64-bit memory address space.  It exists here in order to provide a
    // consistent interface.
    void set_mem_size(unsigned size)
    {
    }
    
    // Note: We used to cache the last page to make this faster, but this seems
    // dangerous and not really workable with a multi-threaded kernel.  So, now
    // it gets looked up each time. 
    //
    // To avoid this overhead, call the read/write functions which take a
    // last_page reference.  This will avoid the secondary lookup.
    byte_t *last_page(addr_t addr) { return getPage(addr) + SparseMemory::pageOffset(addr); };


    // For non-thread-safe memories, check and update the cache.  Otherwise, use
    // the base version of this routine which directly accesses the tree.
#   ifndef THREAD_SAFE_MEMORY
    
    // If the index of the incoming address matches that of the stored
    // last-index, then return the cached page, otherwise request the
    // page from the storage object.
    byte_t *getPage(addr_t addr)
    {
      byte_t *p;
      unsigned cix = cache_index(addr);
      if ( (p = _cache[cix].match(addr)) ) {
        return p;
      } else {       
        // We only need to lock if we don't match in the cache.
        return update_page_cache(addr,cix);
      }
    };

#   else

    using Base::getPage;

#   endif    

    //
    // Basic read/write interface for this memory.
    //

    void readpage(byte_t *t,size_t n,addr_t addr,addr_t crit_addr)
    {
      EXTRA_SCOPED_LOCK;
      byte_pagecopy(t,0,getPage(addr),SparseMemory::pageOffset(addr),n);
    }

    uint64_t read64(addr_t addr)
    {
      EXTRA_SCOPED_LOCK;
      return byte_read64(getPage(addr),SparseMemory::pageOffset(addr));
    }

    uint32_t read32(addr_t addr)
    {
      EXTRA_SCOPED_LOCK;
      return byte_read32(getPage(addr),SparseMemory::pageOffset(addr));
    }

    uint16_t read16(addr_t addr)
    {
      EXTRA_SCOPED_LOCK;
      return byte_read16(getPage(addr),SparseMemory::pageOffset(addr));
    }

    uint32_t read8(addr_t addr)
    {
      EXTRA_SCOPED_LOCK;
      return byte_read8(getPage(addr),SparseMemory::pageOffset(addr));
    }

    uint32_t read0(addr_t addr)
    {
      return 0;
    }

    void fillpage(unsigned char c,size_t n,addr_t addr)
    {
      EXTRA_SCOPED_LOCK;
      byte_pagefill(getPage(addr),SparseMemory::pageOffset(addr),c,n);
    }

    void writepage(byte_t *s,size_t n,addr_t addr,addr_t crit_addr)
    {
      EXTRA_SCOPED_LOCK;
      byte_pagecopy(getPage(addr),SparseMemory::pageOffset(addr),s,0,n);
    }

    void write64(addr_t addr,uint64_t orig_value,unsigned nb)
    {
      EXTRA_SCOPED_LOCK;
      byte_write64(getPage(addr),SparseMemory::pageOffset(addr),orig_value,nb);
    }

    void write32(addr_t addr,uint32_t orig_value,unsigned nb)
    {
      EXTRA_SCOPED_LOCK;
      byte_write32(getPage(addr),SparseMemory::pageOffset(addr),orig_value,nb);
    }

    void write16(addr_t addr,uint16_t orig_value,unsigned nb)
    {
      EXTRA_SCOPED_LOCK;
      byte_write16(getPage(addr),SparseMemory::pageOffset(addr),orig_value,nb);
    }

    void write8(addr_t addr,uint8_t orig_value)
    {
      EXTRA_SCOPED_LOCK;
      byte_write8(getPage(addr),SparseMemory::pageOffset(addr),orig_value);
    }

    void write0(addr_t addr,uint8_t orig_value)
    {
    }

    //
    // These are variants of the above which also return a pointer to the page
    // object, for use by a DMI model.  This way, you don't have to call
    // last_page(), which should help performance with a multi-threaded model.
    //

    // We have to pass the base page and the offset into each of the byte-read
    // routines in order for the data-invariant routines to work.  They need the
    // offset in order to properly swizzle the address.
#   define GET_PAGE  byte_t *page = getPage(addr); \
                     last_page = page + SparseMemory::pageOffset(addr);
    
    void readpage(byte_t *t,size_t n,addr_t addr,addr_t crit_addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_pagecopy(t,0,page,SparseMemory::pageOffset(addr),n);
    }

    uint64_t read64(addr_t addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      return byte_read64(page,SparseMemory::pageOffset(addr));
    }

    uint32_t read32(addr_t addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      return byte_read32(page,SparseMemory::pageOffset(addr));
    }

    uint16_t read16(addr_t addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      return byte_read16(page,SparseMemory::pageOffset(addr));
    }

    uint32_t read8(addr_t addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      return byte_read8(page,SparseMemory::pageOffset(addr));
    }

    uint32_t read0(addr_t addr, byte_t* &last_page)
    {
      last_page = 0;
      return 0;
    }

    void fillpage(unsigned char c,size_t n,addr_t addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_pagefill(page,SparseMemory::pageOffset(addr),c,n);
    }

    void writepage(byte_t *s,size_t n,addr_t addr,addr_t crit_addr, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_pagecopy(page,SparseMemory::pageOffset(addr),s,0,n);
    }

    void write64(addr_t addr,uint64_t orig_value,unsigned nb, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_write64(page,SparseMemory::pageOffset(addr),orig_value,nb);
    }

    void write32(addr_t addr,uint32_t orig_value,unsigned nb, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_write32(page,SparseMemory::pageOffset(addr),orig_value,nb);
    }

    void write16(addr_t addr,uint16_t orig_value,unsigned nb, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_write16(page,SparseMemory::pageOffset(addr),orig_value,nb);
    }

    void write8(addr_t addr,uint8_t orig_value, byte_t* &last_page)
    {
      EXTRA_SCOPED_LOCK;
      GET_PAGE;
      byte_write8(page,SparseMemory::pageOffset(addr),orig_value);
    }

    void write0(addr_t addr,uint8_t orig_value, byte_t* &last_page)
    {
      last_page = 0;
    }
   
    
  private:

    //
    // We don't use the cache for thread-safe memories b/c it would be a
    // read-modify-write memory which would require locking in order to update.
    // Users should use an external DMI cache so that the caching is done within
    // the core's own thread.
    //
#   ifdef THREAD_SAFE_MEMORY

    void resetCache()
    {
    }

#   else    
    
    enum {
      CacheSize = 4096,
      CacheMask = (CacheSize - 1),
      CacheShift = Enums::Shift,
    };
    
    unsigned cache_index(addr_t addr) const
    {
      return ( (addr >> CacheShift) & CacheMask);
    }

    ATTRIBUTE_NOINLINE byte_t *update_page_cache(addr_t addr,unsigned cix) ATTRIBUTE_USED
    {     
      return _cache[cix].update(addr,Base::getPage(addr));
    }

    void resetCache()
    {
      // Erase all entries in the cache.
      ForRange(CacheSize,i) {
        _cache[i].reset();
      }
      // Set up the 0 page just so that it's there.
      getPage(0);
    }

    struct CacheItem {
      addr_t  _addr;  // Stores index of last page.
      byte_t   *_page;  // Stores the last page.

      CacheItem() { reset(); };

      // When we reset an element, we set the address to an impossible value (1)
      // so that it can never match.  This is an impossible value b/c we always
      // compare page numbers, so the offsets are always 0.
      void reset() {
        _page = 0;
        _addr = 1;
      }

      byte_t *match(addr_t addr) {
        if ( (addr & ~PageMask) == _addr) {
          return _page;
        } else {
          return 0;
        }
      }

      byte_t *update(addr_t addr,byte_t *page)
      {
        _addr = addr & ~PageMask;
        return (_page = page);
      }

    };

    CacheItem    _cache[CacheSize];
    
#   endif    
  };


  //
  // This is a simple, block-based memory, with a run-time selected size.  It
  // can be used as an alternative to the sparse memory when a higher-level of
  // performance is required.
  //
  struct BlockMemory {

    // Default memory size, if a block memory is being used.
    enum { DefaultMemorySize = 65536 };

    // The memory will be invalid until a memory size is set.
    BlockMemory() : 
      _mem_size(0),
      _mem(0)
    {
      set_mem_size(DefaultMemorySize);
    }

    void set_mem_size(unsigned size);

    byte_t *last_page(addr_t addr) const { return _mem + mask_addr(addr); };

    void reset()
    {
      if (_mem_size == 0) {
        RError("Memory size not set.");
      }
      memset(_mem,0,_mem_size);
    }

    void readpage(byte_t *t,size_t n,addr_t addr,addr_t crit_addr)
    {
      byte_pagecopy(t,0,_mem,mask_addr(addr),n);
    }

    uint64_t read64(addr_t addr)
    {
      return byte_read64(_mem,mask_addr(addr));
    }

    uint32_t read32(addr_t addr)
    {
      return byte_read32(_mem,mask_addr(addr));
    }

    uint16_t read16(addr_t addr)
    {
      return byte_read16(_mem,mask_addr(addr));
    }

    uint32_t read8(addr_t addr)
    {
      return byte_read8(_mem,mask_addr(addr));
    }

    uint32_t read0(addr_t addr)
    {
      return 0;
    }

    void fillpage(unsigned char c,size_t n,addr_t addr)
    {
      byte_pagefill(_mem,mask_addr(addr),c,n);
    }

    void writepage(byte_t *s,size_t n,addr_t addr,addr_t crit_addr)
    {
      byte_pagecopy(_mem,mask_addr(addr),s,0,n);
    }

    void write64(addr_t addr,uint64_t orig_value,unsigned nb)
    {
      byte_write64(_mem,mask_addr(addr),orig_value,nb);
    }

    void write32(addr_t addr,uint32_t orig_value,unsigned nb)
    {
      byte_write32(_mem,mask_addr(addr),orig_value,nb);
    }

    void write16(addr_t addr,uint16_t orig_value,unsigned nb)
    {
      byte_write16(_mem,mask_addr(addr),orig_value,nb);
    }

    void write8(addr_t addr,uint8_t orig_value)
    {
      byte_write8(_mem,mask_addr(addr),orig_value);
    }

    void write0(addr_t addr,uint8_t orig_value)
    {
    }

  private:

    addr_t mask_addr(addr_t addr) const 
    {
      return addr & _mem_mask;
    }

    addr_t    _mem_mask;    // For making sure addresses are within bounds.
    unsigned  _mem_size;    // Size of the memory in bytes.
    byte_t     *_mem;       // The memory data array.

  };


  //
  // This is a simple memory class used for internal memories.  The size is
  // fixed as part of the type.
  //
  template<size_t MemorySize>
  struct SimpleMemory {

    SimpleMemory() :
      _mem(new byte_t[MemorySize])
    {
      memset(_mem,0,MemorySize);
    }

    ~SimpleMemory()
    {
      delete [] _mem;
    }

    uint64_t read64(addr_t ra) const { return byte_read64(_mem,ra); };
    uint32_t read32(addr_t ra) const { return byte_read32(_mem,ra); };
    uint16_t read16(addr_t ra) const { return byte_read16(_mem,ra); };
    uint8_t  read8 (addr_t ra) const { return byte_read8 (_mem,ra); };
    uint8_t  read0 (addr_t ra) const { return 0; };
  
    void write64(addr_t ra,uint64_t v,unsigned nb = BytesPerDW)   { byte_write64(_mem,ra,v,nb); };
    void write32(addr_t ra,uint32_t v,unsigned nb = BytesPerWord) { byte_write32(_mem,ra,v,nb); };
    void write16(addr_t ra,uint16_t v,unsigned nb = BytesPerHW)   { byte_write16(_mem,ra,v,nb); };
    void write8 (addr_t ra,uint8_t  v)                            { byte_write8 (_mem,ra,v); };
    void write0 (addr_t ra,uint8_t  v)                            { };

  private:
    byte_t    *_mem;
  };


  // Encapsulates the memory accesses and is templated upon a translation type
  // so that instruction/data read/write translations can be different.  The
  // interleave parameters specify whether translations and reads/writes should
  // be interleaved, or if all translations should occur first, followed by the
  // read or write.
  //
  // Translate:   Object to do translation.
  //
  // CacheAccess: Memory or cache object which will do the read or write.
  //
  // Logger:      Object to log activity.
  //
  // Interleaved: Whether memory accesses should be interleaved with translations, 
  //              or if all translations should occur first.
  //
  // SplitSize: Granularity size for splitting accesses, in bytes.  We currently
  //              only support 4 or 8.  If 4, then we'll split an access on a
  //              32-bit boundary for 32-bit and 16-bit accesses.  64-bit
  //              accesses are still split on a 64-bit boundary.  If 8, then we
  //              only split on a 64-bit boundary, meaning that a 32-bit access
  //              within a double-word will not be split.  The default is 3
  //              (32-bit).
  //
  template <typename Translate,typename CacheAccess,typename Logger,bool Interleaved,int SplitSize = 4>
  struct MemoryAccess {

    uint64_t read64_helper_noninterleaved(Translate translate,Logger logger,CacheAccess cache,addr_t ea,addr_t ra,addr_t ea2,addr_t ra2)
    {
      uint64_t val1 = cache.read64(0,ea,ra);
      logger(false,0,ea,ra,8,0);
      uint32_t woffset = cache.misaligned_read_offset64(ea);
      translate.misaligned_read(ea2,ra2);
      uint64_t val2 = cache.read64(1,ea2,ra2);
      return ( (val1 & (0xffffffffffffffffull << (woffset*8)) ) | (val2 >> ((BytesPerDW-woffset)*8)) );
    }

   uint64_t read64_noninterleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      addr_t ea2,ra2;

      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);

      logger(true,0,ea,ra,8,0);
      switch (ea & DWMask) {
      case 0: {
        // Aligned case- only a single read is required.
        translate.pre_read(ea,ra);
        logger(true,0,ea+4,ra+4,8,0);
        uint64_t val = cache.read64(0,ea,ra);
        logger(false,0,ea,ra,8,0);
        logger(false,0,ea+4,ra+4,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      case 1:
      case 2:
      case 3: {
        // Misaligned case, where a page crossing will occur on
        // the third word.
        logger(true,0,ea+4,ra+4,8,0);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_read(ea,ra);
        logger(true,1,ea2,ra2,8,0);
        uint64_t val = read64_helper_noninterleaved(translate,logger,cache,ea,ra,ea2,ra2);
        logger(false,0,ea+4,ra+4,8,0);
        logger(false,1,ea2,ra2,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      case 4: {
        // Misaligned case, where a page crossing will occur
        // on the second word, but no third access exists.
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_read(ea,ra);
        logger(true,1,ea2,ra2,8,0);
        uint64_t val = read64_helper_noninterleaved(translate,logger,cache,ea,ra,ea2,ra2);
        logger(false,1,ea2,ra2,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      case 5:
      case 6:
      default: {
        // Misaligned case, where a page crossing will occur
        // on the second word.
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_read(ea,ra);
        logger(true,1,ea2,ra2,8,0);
        logger(true,1,ea2+4,ra2+4,8,0);
        uint64_t val = read64_helper_noninterleaved(translate,logger,cache,ea,ra,ea2,ra2);
        logger(false,1,ea2,ra2,8,0);
        logger(false,1,ea2+4,ra2+4,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      }
    }

    uint64_t read64_helper_pre_interleaved(Logger logger,CacheAccess cache,addr_t ea,addr_t ra)
    {
      uint64_t val1 = cache.read64(0,ea,ra);
      logger(false,0,ea,ra,8,0);
      return val1;
    }

    uint64_t read64_helper_interleaved(Translate translate,Logger logger,CacheAccess cache,uint64_t val1,addr_t ea,addr_t ea2,addr_t ra2)
    {
      uint32_t woffset = cache.misaligned_read_offset64(ea);
      translate.misaligned_read(ea2,ra2);
      uint64_t val2 = cache.read64(1,ea2,ra2);
      return ( (val1 & (0xffffffffffffffffull << (woffset*8)) ) | (val2 >> ((BytesPerDW-woffset)*8)) );
    }

   uint64_t read64_interleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      addr_t ea2,ra2;

      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);

      logger(true,0,ea,ra,8,0);
      switch (ea & DWMask) {
      case 0: {
        // Aligned case- only a single read is required.
        logger(true,0,ea+4,ra+4,8,0);
        uint64_t val = cache.read64(0,ea,ra);
        logger(false,0,ea,ra,8,0);
        logger(false,0,ea+4,ra+4,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      case 1:
      case 2:
      case 3: {
        // Misaligned case, where a page crossing will occur on
        // the third word.
        logger(true,0,ea+4,ra+4,8,0);
        uint64_t val1 = read64_helper_pre_interleaved(logger,cache,ea,ra);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        logger(true,1,ea2,ra2,8,0);
        uint64_t val = read64_helper_interleaved(translate,logger,cache,val1,ea,ea2,ra2);
        logger(false,0,ea+4,ra+4,8,0);
        logger(false,1,ea2,ra2,8,0);
        translate.post_read(ea,ra);
        return val;          
      }
      case 4: {
        // Misaligned case, where a page crossing will occur
        // on the second word, but no third access exists.
        uint64_t val1 = read64_helper_pre_interleaved(logger,cache,ea,ra);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        logger(true,1,ea2,ra2,8,0);
        uint64_t val = read64_helper_interleaved(translate,logger,cache,val1,ea,ea2,ra2);
        logger(false,1,ea2,ra2,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      case 5:
      case 6:
      default: {
        // Misaligned case, where a page crossing will occur
        // on the second word.
        uint64_t val1 = read64_helper_pre_interleaved(logger,cache,ea,ra);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        logger(true,1,ea2,ra2,8,0);
        logger(true,1,ea2+4,ra2+4,8,0);
        uint64_t val = read64_helper_interleaved(translate,logger,cache,val1,ea,ea2,ra2);
        logger(false,1,ea2,ra2,8,0);
        logger(false,1,ea2+4,ra2+4,8,0);
        translate.post_read(ea,ra);
        return val;
      }
      }
    }

    uint64_t read64(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      if (Interleaved) {
        return read64_interleaved(translate,cache,logger,ea);
      } else {
        return read64_noninterleaved(translate,cache,logger,ea);
      }
    }

    uint32_t misaligned_read32_interleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint32_t val1)
    {
      addr_t ea2 = translate.mask_ea((ea | WordMask) + 1);
      addr_t ra2 = translate(ea2,1);
      uint32_t woffset = cache.misaligned_read_offset32(ea);
      translate.misaligned_read(ea2,ra2);
      logger(true,1,ea2,ra2,4,0);
      uint32_t val2 = cache.read32(1,ea2,ra2);
      logger(false,1,ea2,ra2,4,0);
      translate.post_read(ea,ra);
      return ( (val1 & (0xffffffff << (woffset*8)) ) | (val2 >> ((BytesPerWord-woffset)*8)) );
    }
    
    void misaligned_in_dw_read32(Translate translate,Logger logger,addr_t ea,addr_t ra)
    {
      addr_t ea2 = (ea | WordMask) + 1;
      addr_t ra2 = (ra | WordMask) + 1;
      logger(false,1,ea2,ra2,4,0);
      translate.post_read(ea,ra);
    }

    uint32_t read32_interleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);
      logger(true,0,ea,ra,4,0);
      uint32_t val1 = cache.read32(0,ea,ra);
      logger(false,0,ea,ra,4,0);
      // Template parameter, therefore conditional will be folded out.
      if (SplitSize == 8) {
        // 64-bit split.  If it is misaligned, then handle it like any other
        // misaligned access.  However, if it's aligned within the double-word,
        // then we need to log it as two accesses, but only do a single access.
        switch (ea & DWMask) {
        case 0:
          // Normal, aligned access.
          translate.post_read(ea,ra);
          return val1;
        case 1:
        case 2:
        case 3:
          // Misaligned w/in the double-word.
          misaligned_in_dw_read32(translate,logger,ea,ra);
          return val1;
        case 4:
          // Normal, aligned access.
          translate.post_read(ea,ra);
          return val1;
        default:
          // Misaligned off the end of the double-word.
          return misaligned_read32_interleaved(translate,cache,logger,ea,ra,val1);
        }
      } else {
        // The default is 32-bit.
        if (ea & WordMask) {
          return misaligned_read32_interleaved(translate,cache,logger,ea,ra,val1);
        } else {
          translate.post_read(ea,ra);
          return val1;
        }
      }
    }

    uint32_t misaligned_read32_noninterleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra)
    {
      addr_t ea2 = translate.mask_ea((ea | WordMask) + 1);
      addr_t ra2 = translate(ea2,1);
      translate.pre_read(ea,ra);
      logger(true,0,ea,ra,4,0);
      uint32_t val1 = cache.read32(0,ea,ra);
      logger(false,0,ea,ra,4,0);
      uint32_t woffset = cache.misaligned_read_offset32(ea);
      translate.misaligned_read(ea2,ra2);
      logger(true,1,ea2,ra2,4,0);
      uint32_t val2 = cache.read32(1,ea2,ra2);
      logger(false,1,ea2,ra2,4,0);
      translate.post_read(ea,ra);
      return ( (val1 & (0xffffffff << (woffset*8)) ) | (val2 >> ((BytesPerWord-woffset)*8)) );
    }

    uint32_t read32_noninterleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);
      // Template parameter, therefore conditional will be folded out.
      if (SplitSize == 8) {
        // 64-bit split.  If it is misaligned, then handle it like any other
        // misaligned access.  However, if it's aligned within the double-word,
        // then we need to log it as two accesses, but only do a single access.
        switch (ea & DWMask) {
        case 0: {
          // Normal, aligned access.
          translate.pre_read(ea,ra);
          logger(true,0,ea,ra,4,0);
          uint32_t val1 = cache.read32(0,ea,ra);
          logger(false,0,ea,ra,4,0);
          translate.post_read(ea,ra);
          return val1;
        }
        case 1:
        case 2:
        case 3: {
          // Misaligned w/in the double-word.
          translate.pre_read(ea,ra);
          logger(true,0,ea,ra,4,0);
          uint32_t val1 = cache.read32(0,ea,ra);
          logger(false,0,ea,ra,4,0);
          misaligned_in_dw_read32(translate,logger,ea,ra);
          return val1;
        }
        case 4: {
          // Normal, aligned access.
          translate.pre_read(ea,ra);
          logger(true,0,ea,ra,4,0);
          uint32_t val1 = cache.read32(0,ea,ra);
          logger(false,0,ea,ra,4,0);
          translate.post_read(ea,ra);
          return val1;
        }
        default:
          // Misaligned off the end of the double-word.
          return misaligned_read32_noninterleaved(translate,cache,logger,ea,ra);
        }
      } else {
        // The default is 32-bit.
        if (ea & WordMask) {
          return misaligned_read32_noninterleaved(translate,cache,logger,ea,ra);
        } else {
          translate.pre_read(ea,ra);
          logger(true,0,ea,ra,4,0);
          uint32_t val1 = cache.read32(0,ea,ra);
          logger(false,0,ea,ra,4,0);
          translate.post_read(ea,ra);
          return val1;
        }
      }
    }

    uint64_t read32(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      if (Interleaved) {
        return read32_interleaved(translate,cache,logger,ea);
      } else {
        return read32_noninterleaved(translate,cache,logger,ea);
      }
    }


    uint16_t misaligned_read16_interleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra)
    {
      logger(true,0,ea,ra,2,0);
      uint16_t val1 = cache.read16(0,ea,ra);
      logger(false,0,ea,ra,2,0);
      addr_t ea2 = translate.mask_ea((ea | HWMask) + 1);
      addr_t ra2 = translate(ea2,1);
      uint32_t woffset = cache.misaligned_read_offset16(ea);
      translate.misaligned_read(ea2,ra2);
      logger(true,1,ea2,ra2,2,0);
      uint16_t val2 = cache.read16(1,ea2,ra2);
      logger(false,1,ea2,ra2,2,0);
      translate.post_read(ea,ra);
      return ( (val1 & (0xffff << (woffset*8)) ) | (val2 >> ((BytesPerHW-woffset)*8)) );
    }

    uint16_t misaligned_in_dw_read16(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra)
    {
      translate.pre_read(ea,ra);
      logger(true,0,ea,ra,2,0);
      uint16_t val = cache.read16(0,ea,ra);
      logger(false,0,ea,ra,2,0);
      addr_t ea2 = (ea | HWMask) + 1;
      addr_t ra2 = (ra | HWMask) + 1;
      logger(false,1,ea2,ra2,2,0);
      translate.post_read(ea,ra);
      return val;
    }

    uint16_t aligned_read16(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra)
    {
      translate.pre_read(ea,ra);
      logger(true,0,ea,ra,2,0);
      uint16_t val = cache.read16(0,ea,ra);
      logger(false,0,ea,ra,2,0);
      translate.post_read(ea,ra);
      return val;
    }

    // We're just concerned with a minimum of word-misaligned addresses in this
    // case.  The SplitSize determines whether we care about word-misaligned or
    // double-word misaligned.
    uint16_t read16_interleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);
      if (SplitSize == 8) {
        if ( (ea & DWMask) == 7) {
          // Misaligned on a double-word, so we have to split.
          return misaligned_read16_interleaved(translate,cache,logger,ea,ra);
        } else if ( (ea & WordMask) == 3) {          
          // Misaligned within a word, so only a single access is needed, but we
          // have to log two accesses.
          return misaligned_in_dw_read16(translate,cache,logger,ea,ra);
        } else {
          return aligned_read16(translate,cache,logger,ea,ra);
        }
      } else {
        if ( (ea & WordMask) == 3 ) {
          return misaligned_read16_interleaved(translate,cache,logger,ea,ra);
        } else {
          return aligned_read16(translate,cache,logger,ea,ra);
        }
      }
    }

    uint16_t misaligned_read16_noninterleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra)
    {
      addr_t ea2 = translate.mask_ea((ea | HWMask) + 1);
      addr_t ra2 = translate(ea2,1);
      uint32_t woffset = cache.misaligned_read_offset16(ea);
      translate.pre_read(ea,ra);
      logger(true,0,ea,ra,2,0);
      uint16_t val1 = cache.read16(0,ea,ra);
      logger(false,0,ea,ra,2,0);
      translate.misaligned_read(ea2,ra2);
      logger(true,1,ea2,ra2,2,0);
      uint16_t val2 = cache.read16(1,ea2,ra2);
      logger(false,1,ea2,ra2,2,0);
      translate.post_read(ea,ra);
      return ( (val1 & (0xffff << (woffset*8)) ) | (val2 >> ((BytesPerHW-woffset)*8)) );
    }

    uint16_t read16_noninterleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);
      if (SplitSize == 8) {
        if ( (ea & DWMask) == 7) {
          // Misaligned on a double-word, so we have to split.
          return misaligned_read16_noninterleaved(translate,cache,logger,ea,ra);
        } else if ( (ea & WordMask) == 3) {          
          // Misaligned within a word, so only a single access is needed, but we
          // have to log two accesses.
          return misaligned_in_dw_read16(translate,cache,logger,ea,ra);
        } else {
          return aligned_read16(translate,cache,logger,ea,ra);
        }
      } else {
        if ( (ea & WordMask) == 3 ) {
          return misaligned_read16_noninterleaved(translate,cache,logger,ea,ra);
        } else {
          return aligned_read16(translate,cache,logger,ea,ra);
        }
      }
    }

    uint64_t read16(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      if (Interleaved) {
        return read16_interleaved(translate,cache,logger,ea);
      } else {
        return read16_noninterleaved(translate,cache,logger,ea);
      }
    }

    uint8_t read8(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);
      translate.pre_read(ea,ra);
      logger(true,0,ea,ra,1,0);
      uint8_t val = cache.read8(ea,ra);
      logger(false,0,ea,ra,1,0);
      translate.post_read(ea,ra);
      return val;
    }

    uint8_t read0(Translate translate,CacheAccess cache,Logger logger,addr_t ea)
    {
      ea = translate.mask_ea(ea);
      addr_t ra = translate(ea,0);
      translate.pre_read(ea,ra);
      logger(true,0,ea,ra,0,0);
      cache.read0(ea,ra);
      logger(false,0,ea,ra,0,0);
      translate.post_read(ea,ra);
      return 0;
    }

    void write64_helper_noninterleaved(Translate translate,Logger logger,uint32_t woffset,CacheAccess cache,addr_t ea,addr_t ra,addr_t ea2,addr_t ra2,uint64_t orig_value)
    {
      cache.write64(ea,ra,orig_value,(BytesPerDW-woffset));
      logger(false,0,ea,ra,8,orig_value);
      translate.misaligned_write(ea2,ra2);
      int s2 = ((BytesPerDW-woffset)*8);
      cache.write64(ea2,ra2,(orig_value << s2),woffset);
    }

    //
    // Note that the write functions always take an ra and expect for the
    // initial translation to be done by the caller.  This allows for a hook to
    // be inserted between the translation and the actual memory access.
    //

    void write64_noninterleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint64_t orig_value)
    {
      uint32_t woffset = ea & DWMask;
      addr_t ea2,ra2;

      logger(true,0,ea,ra,8,orig_value);
      switch ( woffset ) {
      case 0:
        // Aligned case- only a single write is required.
        translate.pre_write(ea,ra);
        logger(true,0,ea+4,ra+4,8,orig_value);
        cache.write64(ea,ra,orig_value,BytesPerDW);
        logger(false,0,ea,ra,8,orig_value);
        logger(false,0,ea+4,ra+4,8,orig_value);
        translate.post_write(ea,ra);
        return;
      case 1:
      case 2:
      case 3: {
        // Misaligned case, where a page crossing will occur on
        // the third word.
        logger(true,0,ea+4,ra+4,8,orig_value);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_write(ea,ra);
        logger(true,1,ea2,ra2,8,orig_value);
        write64_helper_noninterleaved(translate,logger,woffset,cache,ea,ra,ea2,ra2,orig_value);
        logger(false,0,ea+4,ra+4,8,orig_value);
        logger(false,1,ea2,ra2,8,orig_value);
        translate.post_write(ea,ra);
        return;
      }
      case 4:
        // Misaligned case, where a page crossing will occur
        // on the second word, but no third access exists.
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_write(ea,ra);
        logger(true,1,ea2,ra2,8,orig_value);
        write64_helper_noninterleaved(translate,logger,woffset,cache,ea,ra,ea2,ra2,orig_value);
        logger(false,1,ea2,ra2,8,orig_value);
        translate.post_write(ea,ra);
        return;
      case 5:
      case 6:
      case 7: {
        // Misaligned case, where a page crossing will occur
        // on the second word.
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_write(ea,ra);
        logger(true,1,ea2,ra2,8,orig_value);
        logger(true,1,ea2+4,ra2+4,8,orig_value);
        write64_helper_noninterleaved(translate,logger,woffset,cache,ea,ra,ea2,ra2,orig_value);
        logger(false,1,ea2,ra2,8,orig_value);
        logger(false,1,ea2+4,ra2+4,8,orig_value);
        translate.post_write(ea,ra);
        return;
      }
      }
    }

    void write64_helper_interleaved1(Translate translate,Logger logger,uint32_t woffset,CacheAccess cache,addr_t ea,addr_t ra,uint64_t orig_value)
    {
      cache.write64(ea,ra,orig_value,(BytesPerDW-woffset));
      logger(false,0,ea,ra,8,orig_value);
    }

    void write64_helper_interleaved2(Translate translate,Logger logger,uint32_t woffset,CacheAccess cache,addr_t ea2,addr_t ra2,uint64_t orig_value)
    {
      translate.misaligned_write(ea2,ra2);
      int s2 = ((BytesPerDW-woffset)*8);
      cache.write64(ea2,ra2,orig_value << s2,woffset);
    }

    //
    // Note that the write functions always take an ra and expect for the
    // initial translation to be done by the caller.  This allows for a hook to
    // be inserted between the translation and the actual memory access.
    //

    void write64_interleaved(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint64_t orig_value)
    {
      uint32_t woffset = ea & DWMask;
      addr_t ea2,ra2;

      logger(true,0,ea,ra,8,orig_value);
      switch ( woffset ) {
      case 0:
        // Aligned case- only a single write is required.
        logger(true,0,ea+4,ra+4,8,orig_value);
        cache.write64(ea,ra,orig_value,BytesPerDW);
        logger(false,0,ea,ra,8,orig_value);
        logger(false,0,ea+4,ra+4,8,orig_value);
        translate.post_write(ea,ra);
        return;
      case 1:
      case 2:
      case 3: {
        // Misaligned case, where a page crossing will occur on
        // the third word.
        logger(true,0,ea+4,ra+4,8,orig_value);
        write64_helper_interleaved1(translate,logger,woffset,cache,ea,ra,orig_value);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        logger(true,1,ea2,ra2,8,orig_value);
        write64_helper_interleaved2(translate,logger,woffset,cache,ea2,ra2,orig_value);
        logger(false,0,ea+4,ra+4,8,orig_value);
        logger(false,1,ea2,ra2,8,orig_value);
        translate.post_write(ea,ra);
        return;
      }
      case 4:
        // Misaligned case, where a page crossing will occur
        // on the second word, but no third access exists.
        write64_helper_interleaved1(translate,logger,woffset,cache,ea,ra,orig_value);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_write(ea,ra);
        logger(true,1,ea2,ra2,8,orig_value);
        write64_helper_interleaved2(translate,logger,woffset,cache,ea2,ra2,orig_value);
        logger(false,1,ea2,ra2,8,orig_value);
        translate.post_write(ea,ra);
        return;
      case 5:
      case 6:
      case 7: {
        // Misaligned case, where a page crossing will occur
        // on the second word.
        write64_helper_interleaved1(translate,logger,woffset,cache,ea,ra,orig_value);
        ea2 = translate.mask_ea((ea | DWMask) + 1);
        ra2 = translate(ea2,1);
        translate.pre_write(ea,ra);
        logger(true,1,ea2,ra2,8,orig_value);
        logger(true,1,ea2+4,ra2+4,8,orig_value);
        write64_helper_interleaved2(translate,logger,woffset,cache,ea2,ra2,orig_value);
        logger(false,1,ea2,ra2,8,orig_value);
        logger(false,1,ea2+4,ra2+4,8,orig_value);
        translate.post_write(ea,ra);
        return;
      }
      }
    }

    void write64(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint64_t orig_value)
    {
      if (Interleaved) {
        return write64_interleaved(translate,cache,logger,ea,ra,orig_value);
      } else {
        return write64_noninterleaved(translate,cache,logger,ea,ra,orig_value);
      }
    }

    void misaligned_write32(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint32_t orig_value)
    {
      uint32_t woffset = ea & WordMask;
      if (Interleaved) {
        logger(true,0,ea,ra,4,orig_value);
        cache.write32(ea,ra,orig_value,(BytesPerWord - woffset));
        logger(false,0,ea,ra,4,orig_value);
      }
      addr_t ea2 = translate.mask_ea((ea | WordMask) + 1);
      addr_t ra2 = translate(ea2,1);
      if (!Interleaved) {
        translate.pre_write(ea,ra);
        logger(true,0,ea,ra,4,orig_value);
        cache.write32(ea,ra,orig_value,(BytesPerWord - woffset));
        logger(false,0,ea,ra,4,orig_value);
      }
      translate.misaligned_write(ea2,ra2);
      logger(true,1,ea2,ra2,4,orig_value);
      int s2 = ((BytesPerWord-woffset)*8);
      cache.write32(ea2,ra2,(orig_value << s2),woffset);
      logger(false,1,ea2,ra2,4,orig_value);
    }

    void misaligned_in_dw_write32(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint32_t orig_value)
    {
      aligned_write32(translate,cache,logger,ea,ra,orig_value);
      addr_t ea2 = (ea | WordMask) + 1;
      addr_t ra2 = (ra | WordMask) + 1;
      logger(false,1,ea2,ra2,4,orig_value);
    }

    void aligned_write32(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint32_t orig_value)
    {
      translate.pre_write(ea,ra);
      logger(true,0,ea,ra,4,orig_value);
      cache.write32(ea,ra,orig_value,BytesPerWord);
      logger(false,0,ea,ra,4,orig_value);
    }

    void write32(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint32_t orig_value)
    {
      if (SplitSize == 8) {
        // 64-bit split.  If it is misaligned, then handle it like any other
        // misaligned access.  However, if it's aligned within the double-word,
        // then we need to log it as two accesses, but only do a single access.
        switch (ea & DWMask) {
        case 0:
          // Normal, aligned access.
          aligned_write32(translate,cache,logger,ea,ra,orig_value);
          break;
        case 1:
        case 2:
        case 3:
          // Misaligned w/in the double-word.
          misaligned_in_dw_write32(translate,cache,logger,ea,ra,orig_value);
          break;
        case 4:
          // Normal, aligned access.
          aligned_write32(translate,cache,logger,ea,ra,orig_value);
          break;
        default:
          // Misaligned off the end of the double-word.
          misaligned_write32(translate,cache,logger,ea,ra,orig_value);
        }
      } else {
        if ( ea & WordMask) {
          // Misaligned case- two writes are required.
          misaligned_write32(translate,cache,logger,ea,ra,orig_value);
        } else {
          // Aligned case- only a single write is required.
          aligned_write32(translate,cache,logger,ea,ra,orig_value);
        }
      }
      translate.post_write(ea,ra);
    }


    void misaligned_write16(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint16_t orig_value)
    {
      if (Interleaved) {
        logger(true,0,ea,ra,2,orig_value);
        cache.write16(ea,ra,orig_value,1);
        logger(false,0,ea,ra,2,orig_value);
      }
      addr_t ea2 = translate.mask_ea((ea | HWMask) + 1);
      addr_t ra2 = translate(ea2,1);
      // Misaligned case- two writes are required.
      if (!Interleaved) {
        translate.pre_write(ea,ra);
        logger(true,0,ea,ra,2,orig_value);
        cache.write16(ea,ra,orig_value,1);
        logger(false,0,ea,ra,2,orig_value);
      }
      translate.misaligned_write(ea2,ra2);
      logger(true,1,ea2,ra2,2,orig_value);
      cache.write16(ea2,ra2,orig_value << 8,1);
      logger(false,1,ea2,ra2,2,orig_value);
    }

    void misaligned_in_dw_write16(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint32_t orig_value)
    {
      aligned_write16(translate,cache,logger,ea,ra,orig_value);
      addr_t ea2 = (ea | WordMask) + 1;
      addr_t ra2 = (ra | WordMask) + 1;
      logger(false,1,ea2,ra2,2,orig_value);
    }

    void aligned_write16(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint16_t orig_value)
    {
      translate.pre_write(ea,ra);
      logger(true,0,ea,ra,2,orig_value);
      cache.write16(ea,ra,orig_value,2);
      logger(false,0,ea,ra,2,orig_value);
    }

    // We're just concerned with word-crossings, so we only care if we're misaligned
    // on a word-boundary.
    void write16(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint16_t orig_value)
    {
      if (SplitSize == 8) {
        if ( (ea & DWMask) == 7) {
          // Misaligned off the end of the double-word.
          misaligned_write16(translate,cache,logger,ea,ra,orig_value);
        } else if ( (ea & WordMask) == 3) {
          misaligned_in_dw_write16(translate,cache,logger,ea,ra,orig_value);
        } else {
          aligned_write16(translate,cache,logger,ea,ra,orig_value);
        }
      } else {
        if ( (ea & WordMask) == 3 ) {
          misaligned_write16(translate,cache,logger,ea,ra,orig_value);
        } else {
          // Aligned case- only a single write is required.
          aligned_write16(translate,cache,logger,ea,ra,orig_value);
        }
      }
      translate.post_write(ea,ra);
    }

    // There's no need for a translate object b/c we can never have a second
    // translation, since a byte cannot cross a page.
    void write8(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint8_t orig_value)
    {
      translate.pre_write(ea,ra);
      logger(true,0,ea,ra,1,orig_value);
      cache.write8(ea,ra,orig_value);
      logger(false,0,ea,ra,1,orig_value);
      translate.post_write(ea,ra);
    }

    // There's no need for a translate object b/c we aren't actually writing anything.
    void write0(Translate translate,CacheAccess cache,Logger logger,addr_t ea,addr_t ra,uint8_t orig_value)
    {
      translate.pre_write(ea,ra);
      logger(true,0,ea,ra,0,0);
      cache.write0(ea,ra,orig_value);
      logger(false,0,ea,ra,0,0);
      translate.post_write(ea,ra);
    }

  };

}

#endif
